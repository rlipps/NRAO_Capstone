{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Natural Language Processing for Text Classification with NLTK and Scikit-learn\n",
    "\n",
    "### Presented by Eduonix!\n",
    "\n",
    "In the project, Getting Started With Natural Language Processing in Python, we learned the basics of tokenizing, part-of-speech tagging, stemming, chunking, and named entity recognition; furthermore, we dove into machine learning and text classification using a simple support vector classifier and a dataset of positive and negative movie reviews. \n",
    "\n",
    "In this tutorial, we will expand on this foundation and explore different ways to improve our text classification results. We will cover and use:\n",
    "\n",
    "* Regular Expressions\n",
    "* Feature Engineering\n",
    "* Multiple scikit-learn Classifiers\n",
    "* Ensemble Methods\n",
    "\n",
    "### 1. Import Necessary Libraries\n",
    "\n",
    "To ensure the necessary libraries are installed correctly and up-to-date, print the version numbers for each library.  This will also improve the reproducibility of our project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.9.16 (main, Mar  8 2023, 04:29:44) \n",
      "[Clang 14.0.6 ]\n",
      "NLTK: 3.8.1\n",
      "Scikit-learn: 1.2.2\n",
      "Pandas: 2.0.2\n",
      "Numpy: 1.23.5\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import nltk\n",
    "import sklearn\n",
    "import pandas\n",
    "import numpy\n",
    "\n",
    "print('Python: {}'.format(sys.version))\n",
    "print('NLTK: {}'.format(nltk.__version__))\n",
    "print('Scikit-learn: {}'.format(sklearn.__version__))\n",
    "print('Pandas: {}'.format(pandas.__version__))\n",
    "print('Numpy: {}'.format(numpy.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Load the Dataset\n",
    "\n",
    "Now that we have ensured that our libraries are installed correctly, let's load the data set as a Pandas DataFrame. Furthermore, let's extract some useful information such as the column information and class distributions. \n",
    "\n",
    "The data set we will be using comes from the UCI Machine Learning Repository.  It contains over 5000 SMS labeled messages that have been collected for mobile phone spam research. It can be downloaded from the following URL:\n",
    "\n",
    "https://archive.ics.uci.edu/ml/datasets/sms+spam+collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load the dataset of SMS messages\n",
    "df = pd.read_csv('https://raw.githubusercontent.com/rlipps/NRAO_Capstone/main/data/nrao_projects.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4528 entries, 0 to 4527\n",
      "Data columns (total 4 columns):\n",
      " #   Column            Non-Null Count  Dtype \n",
      "---  ------            --------------  ----- \n",
      " 0   project_code      4528 non-null   object\n",
      " 1   project_title     4528 non-null   object\n",
      " 2   project_abstract  4527 non-null   object\n",
      " 3   fs_type           4528 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 141.6+ KB\n",
      "None\n",
      "     project_code                                      project_title  \\\n",
      "0  2018.1.01205.L  Fifty AU STudy of the chemistry in the disk/en...   \n",
      "1  2022.1.00316.L  COMPASS: Complex Organic Molecules in Protosta...   \n",
      "2  2017.1.00161.L  ALCHEMI: the ALMA Comprehensive High-resolutio...   \n",
      "3  2021.1.01616.L  ALMA JELLY - Survey of Nearby Jellyfish and Ra...   \n",
      "4  2021.1.00869.L  Bulge symmetry or not? The hidden dynamics of ...   \n",
      "\n",
      "                                    project_abstract fs_type  \n",
      "0  The huge variety of planetary systems discover...    line  \n",
      "1  The emergence of complex organic molecules in ...    line  \n",
      "2  A great variety in gas composition is observed...    line  \n",
      "3  We propose the first ever statistical survey o...    line  \n",
      "4  A radio survey of red giant SiO sources in the...    line  \n"
     ]
    }
   ],
   "source": [
    "# print useful information about the dataset\n",
    "print(df.info())\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_code</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_abstract</th>\n",
       "      <th>fs_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018.1.01205.L</td>\n",
       "      <td>Fifty AU STudy of the chemistry in the disk/en...</td>\n",
       "      <td>The huge variety of planetary systems discover...</td>\n",
       "      <td>line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022.1.00316.L</td>\n",
       "      <td>COMPASS: Complex Organic Molecules in Protosta...</td>\n",
       "      <td>The emergence of complex organic molecules in ...</td>\n",
       "      <td>line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017.1.00161.L</td>\n",
       "      <td>ALCHEMI: the ALMA Comprehensive High-resolutio...</td>\n",
       "      <td>A great variety in gas composition is observed...</td>\n",
       "      <td>line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021.1.01616.L</td>\n",
       "      <td>ALMA JELLY - Survey of Nearby Jellyfish and Ra...</td>\n",
       "      <td>We propose the first ever statistical survey o...</td>\n",
       "      <td>line</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021.1.00869.L</td>\n",
       "      <td>Bulge symmetry or not? The hidden dynamics of ...</td>\n",
       "      <td>A radio survey of red giant SiO sources in the...</td>\n",
       "      <td>line</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     project_code                                      project_title  \\\n",
       "0  2018.1.01205.L  Fifty AU STudy of the chemistry in the disk/en...   \n",
       "1  2022.1.00316.L  COMPASS: Complex Organic Molecules in Protosta...   \n",
       "2  2017.1.00161.L  ALCHEMI: the ALMA Comprehensive High-resolutio...   \n",
       "3  2021.1.01616.L  ALMA JELLY - Survey of Nearby Jellyfish and Ra...   \n",
       "4  2021.1.00869.L  Bulge symmetry or not? The hidden dynamics of ...   \n",
       "\n",
       "                                    project_abstract fs_type  \n",
       "0  The huge variety of planetary systems discover...    line  \n",
       "1  The emergence of complex organic molecules in ...    line  \n",
       "2  A great variety in gas composition is observed...    line  \n",
       "3  We propose the first ever statistical survey o...    line  \n",
       "4  A radio survey of red giant SiO sources in the...    line  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "fs_type\n",
       "line         3628\n",
       "continuum     900\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check class distribution\n",
    "df.fs_type.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = df.fs_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       line\n",
       "1       line\n",
       "2       line\n",
       "3       line\n",
       "4       line\n",
       "        ... \n",
       "4523    line\n",
       "4524    line\n",
       "4525    line\n",
       "4526    line\n",
       "4527    line\n",
       "Name: fs_type, Length: 4528, dtype: object"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Preprocess the Data\n",
    "\n",
    "Preprocessing the data is an essential step in natural language process. In the following cells, we will convert our class labels to binary values using the LabelEncoder from sklearn, replace email addresses, URLs, phone numbers, and other symbols by using regular expressions, remove stop words, and extract word stems.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# convert class labels to binary values, 0 = ham and 1 = spam\n",
    "encoder = LabelEncoder()\n",
    "Y = encoder.fit_transform(classes)\n",
    "\n",
    "print(Y[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>project_code</th>\n",
       "      <th>project_title</th>\n",
       "      <th>project_abstract</th>\n",
       "      <th>fs_type</th>\n",
       "      <th>both</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2018.1.01205.L</td>\n",
       "      <td>Fifty AU STudy of the chemistry in the disk/en...</td>\n",
       "      <td>The huge variety of planetary systems discover...</td>\n",
       "      <td>line</td>\n",
       "      <td>Fifty AU STudy of the chemistry in the disk/en...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022.1.00316.L</td>\n",
       "      <td>COMPASS: Complex Organic Molecules in Protosta...</td>\n",
       "      <td>The emergence of complex organic molecules in ...</td>\n",
       "      <td>line</td>\n",
       "      <td>COMPASS: Complex Organic Molecules in Protosta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2017.1.00161.L</td>\n",
       "      <td>ALCHEMI: the ALMA Comprehensive High-resolutio...</td>\n",
       "      <td>A great variety in gas composition is observed...</td>\n",
       "      <td>line</td>\n",
       "      <td>ALCHEMI: the ALMA Comprehensive High-resolutio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2021.1.01616.L</td>\n",
       "      <td>ALMA JELLY - Survey of Nearby Jellyfish and Ra...</td>\n",
       "      <td>We propose the first ever statistical survey o...</td>\n",
       "      <td>line</td>\n",
       "      <td>ALMA JELLY - Survey of Nearby Jellyfish and Ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2021.1.00869.L</td>\n",
       "      <td>Bulge symmetry or not? The hidden dynamics of ...</td>\n",
       "      <td>A radio survey of red giant SiO sources in the...</td>\n",
       "      <td>line</td>\n",
       "      <td>Bulge symmetry or not? The hidden dynamics of ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     project_code                                      project_title  \\\n",
       "0  2018.1.01205.L  Fifty AU STudy of the chemistry in the disk/en...   \n",
       "1  2022.1.00316.L  COMPASS: Complex Organic Molecules in Protosta...   \n",
       "2  2017.1.00161.L  ALCHEMI: the ALMA Comprehensive High-resolutio...   \n",
       "3  2021.1.01616.L  ALMA JELLY - Survey of Nearby Jellyfish and Ra...   \n",
       "4  2021.1.00869.L  Bulge symmetry or not? The hidden dynamics of ...   \n",
       "\n",
       "                                    project_abstract fs_type  \\\n",
       "0  The huge variety of planetary systems discover...    line   \n",
       "1  The emergence of complex organic molecules in ...    line   \n",
       "2  A great variety in gas composition is observed...    line   \n",
       "3  We propose the first ever statistical survey o...    line   \n",
       "4  A radio survey of red giant SiO sources in the...    line   \n",
       "\n",
       "                                                both  \n",
       "0  Fifty AU STudy of the chemistry in the disk/en...  \n",
       "1  COMPASS: Complex Organic Molecules in Protosta...  \n",
       "2  ALCHEMI: the ALMA Comprehensive High-resolutio...  \n",
       "3  ALMA JELLY - Survey of Nearby Jellyfish and Ra...  \n",
       "4  Bulge symmetry or not? The hidden dynamics of ...  "
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['both']=  df.project_title+ ' '+df.project_abstract\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'COMPASS: Complex Organic Molecules in Protostars with ALMA Spectral Surveys The emergence of complex organic molecules in the interstellar medium is a fundamental puzzle of astrochemistry. Targeted observations with ALMA have opened the door to high-sensitivity spectral surveys over wide bandwidths to elucidate the chemical complexity of young stars in a systematic manner. We propose a Large Program to perform unbiased line surveys in the 279 to 312 GHz frequency range of 11 nearby Solar-type protostars. The targeted protostars are known hosts of complex organic molecules and sample different natal environments and evolutionary stages. The proposed spectral coverage will allow us to unambiguously identify complex organic molecules and their isotopologues and to accurately derive the abundances for species with abundances down to 0.01% relative to methanol. The concerted effort will provide a deep understanding of the complex organic inventories and isotopic ratios depending on the physical environment and evolutionary past. This will move the field forward in disentangling the formation of such molecules under interstellar conditions and ultimately address how much diversity in organic inventories we can expect for emerging planetary systems.'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.both[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    Fifty AU STudy of the chemistry in the disk/en...\n",
      "1    COMPASS: Complex Organic Molecules in Protosta...\n",
      "2    ALCHEMI: the ALMA Comprehensive High-resolutio...\n",
      "3    ALMA JELLY - Survey of Nearby Jellyfish and Ra...\n",
      "4    Bulge symmetry or not? The hidden dynamics of ...\n",
      "5    The ALMA survey to Resolve exoKuiper belt Subs...\n",
      "6    UNveiling the Initial Conditions of high-mass ...\n",
      "7    REBELS: An ALMA Large Program to Discover the ...\n",
      "8    ACES: The ALMA CMZ Exploration Survey The extr...\n",
      "9    The COSMOS High-z ALMA-MIRI Population Survey ...\n",
      "Name: both, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# store the SMS message data\n",
    "abstracts = df.both\n",
    "print(abstracts[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Regular Expressions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make them all strings\n",
    "processed =abstracts.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Properties of the most distant star-forming GMC in the Milky Way The properties of molecular clouds and star formation are expected to be subject to Galaxy-scale variations. In the Outer Galaxy, the importance of the spiral structure is thought to be diminished, with a metallicity gradient observed with galactocentric radius. Despite the importance of location in determining the outcome(s) of the ISM life-cycle, these effects are poorly understood. This leaves extrapolation from our current understanding to molecular cloud and star formation properties in even nearby galaxies uncertain, let alone to the conditions under which the bulk of stars in the Universe formed. We propose to comprehensively map and characterise the molecular gas and dust in the most distant star forming region found to date within our Galaxy, where the metallicity is expected to be significantly below that in the Inner Galaxy and supernovae may be more important than spiral arms. With these data, we will constrain the cloud properties (e.g. clump mass efficiency, column density PDF) at comparable resolution to single-dish observations of nearby star forming regions, thus providing the first step in bridging the gap between the Inner Galaxy and the nearby universe.'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ultra-high resolution imaging of 3C84 3C84 is a prime target for high angular resolution studies of jet formation, due to its proximity and large SMBH mass, which provides a spatial resolution of 20 Rs. Previous 1.3cm RadioAstron space-VLBI imaging and 3mm GMVA maps show a prominent two rail jet, which is anchored in an E-W oriented compact component, perpendicular to the outer jet. With the proposed EHT+ALMA observation we address the question of the physical nature of this elongated jet base and the \"true\" location of the jet apex. EHT imaging with ultra-high angular resolution will allow for a number of questions to be answered: the Faraday depth, magnetic field and particle density can be estimated through the rotation measure. The location of the jet base will be precisely pinpointed. The transverse jet width and nozzle (profile) will allow for a discrimination between magnetic and/or pressure confinement in the jet launching region. Finally, the magnetic field topology and orientation will be constrained by polarisation imaging. These parameters will set important constraints for the future theoretical modelling of BH jet launching.'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[4525]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Ultra-high resolution imaging of 3C84 3C84 is a prime target for high angular resolution studies of jet formation, due to its proximity and large SMBH mass, which provides a spatial resolution of 20 Rs. Previous 1.3cm RadioAstron space-VLBI imaging and 3mm GMVA maps show a prominent two rail jet, which is anchored in an E-W oriented compact component, perpendicular to the outer jet. With the proposed EHT+ALMA observation we address the question of the physical nature of this elongated jet base and the \"true\" location of the jet apex. EHT imaging with ultra-high angular resolution will allow for a number of questions to be answered: the Faraday depth, magnetic field and particle density can be estimated through the rotation measure. The location of the jet base will be precisely pinpointed. The transverse jet width and nozzle (profile) will allow for a discrimination between magnetic and/or pressure confinement in the jet launching region. Finally, the magnetic field topology and orientation will be constrained by polarisation imaging. These parameters will set important constraints for the future theoretical modelling of BH jet launching.'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed[4525]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    punctuation_pattern = r'[^\\w\\s]'\n",
    "    return re.sub(punctuation_pattern, '', text)\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "processed = processed.apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use regular expressions to replace numbers\n",
    "    \n",
    "def replace_numbers_with_word(text, placeholder='numbr'):\n",
    "    # Use a pattern that matches digits followed by any combination of letters\n",
    "    number_pattern = r'\\b\\d+[A-Za-z]*\\b'\n",
    "    \n",
    "    return re.sub(number_pattern, placeholder, text)\n",
    "\n",
    "# Apply the function to the 'text' column\n",
    "processed = processed.apply(replace_numbers_with_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_extra_spaces(text):\n",
    "    return re.sub(r'\\s+', ' ', text).strip()\n",
    "processed = processed.apply(replace_numbers_with_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       fifty au study of the chemistry in the diskenv...\n",
      "1       compass complex organic molecules in protostar...\n",
      "2       alchemi the alma comprehensive highresolution ...\n",
      "3       alma jelly  survey of nearby jellyfish and ram...\n",
      "4       bulge symmetry or not the hidden dynamics of t...\n",
      "                              ...                        \n",
      "4523    a detailed study of the subpc jet of bl lacert...\n",
      "4524    disorder vs order discerning the nature of the...\n",
      "4525    ultrahigh resolution imaging of 3c84 3c84 is a...\n",
      "4526    imaging jet and magnetic field near the spinni...\n",
      "4527    first subparsecscale imaging of the new tev ga...\n",
      "Name: both, Length: 4528, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# change words to lower case - Hello, HELLO, hello are all the same word\n",
    "processed = processed.str.lower()\n",
    "print(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "# remove stop words from text messages\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(\n",
    "    term for term in x.split() if term not in stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove word stems using a Porter stemmer\n",
    "ps = nltk.PorterStemmer()\n",
    "\n",
    "processed = processed.apply(lambda x: ' '.join(\n",
    "    ps.stem(term) for term in x.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generating Features\n",
    "\n",
    "Feature engineering is the process of using domain knowledge of the data to create features for machine learning algorithms. In this project, the words in each text message will be our features.  For this purpose, it will be necessary to tokenize each word.  We will use the 1500 most common words as features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# create bag-of-words\n",
    "all_words = []\n",
    "\n",
    "for message in processed:\n",
    "    words = word_tokenize(message)\n",
    "    for w in words:\n",
    "        all_words.append(w)\n",
    "        \n",
    "all_words = nltk.FreqDist(all_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words: 13814\n",
      "Most common words: [('numbr', 21167), ('observ', 9054), ('galaxi', 7878), ('ga', 6711), ('disk', 6040), ('star', 5897), ('propos', 5313), ('alma', 5109), ('format', 5013), ('molecular', 4600), ('mass', 3464), ('line', 3374), ('dust', 3210), ('studi', 3110), ('emiss', 3097)]\n"
     ]
    }
   ],
   "source": [
    "# print the total number of words and the 15 most common words\n",
    "print('Number of words: {}'.format(len(all_words)))\n",
    "print('Most common words: {}'.format(all_words.most_common(15)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the 1500 most common words as features\n",
    "word_features = list(all_words.keys())[:1500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fifti\n",
      "au\n",
      "studi\n",
      "chemistri\n",
      "diskenvelop\n",
      "system\n",
      "solarlik\n",
      "protostar\n",
      "faust\n",
      "huge\n",
      "varieti\n",
      "planetari\n",
      "discov\n",
      "recent\n",
      "decad\n",
      "like\n",
      "depend\n",
      "earli\n",
      "histori\n",
      "format\n",
      "propos\n",
      "larg\n",
      "program\n",
      "focus\n",
      "specif\n",
      "chemic\n",
      "divers\n",
      "scale\n",
      "numbr\n",
      "planet\n",
      "expect\n",
      "form\n",
      "particular\n",
      "goal\n",
      "project\n",
      "reveal\n",
      "quantifi\n",
      "composit\n",
      "envelopedisk\n",
      "sampl\n",
      "class\n",
      "repres\n",
      "observ\n",
      "larger\n",
      "sourc\n",
      "spatial\n",
      "resolut\n",
      "set\n",
      "molecul\n",
      "abl\n",
      "disentangl\n",
      "compon\n",
      "characteris\n",
      "organ\n",
      "complex\n",
      "probe\n",
      "ioniz\n",
      "structur\n",
      "measur\n",
      "molecular\n",
      "deuter\n",
      "output\n",
      "homogen\n",
      "databas\n",
      "thousand\n",
      "imag\n",
      "differ\n",
      "line\n",
      "speci\n",
      "ie\n",
      "unpreced\n",
      "sourcesurvey\n",
      "provid\n",
      "commun\n",
      "legaci\n",
      "dataset\n",
      "mileston\n",
      "astrochemistri\n",
      "star\n"
     ]
    }
   ],
   "source": [
    "# The find_features function will determine which of the 1500 word features are contained in the review\n",
    "def find_features(message):\n",
    "    words = word_tokenize(message)\n",
    "    features = {}\n",
    "    for word in word_features:\n",
    "        features[word] = (word in words)\n",
    "\n",
    "    return features\n",
    "\n",
    "# Lets see an example!\n",
    "features = find_features(processed[0])\n",
    "for key, value in features.items():\n",
    "    if value == True:\n",
    "        print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now lets do it for all the messages\n",
    "messages = zip(processed, Y)\n",
    "messages = list(messages)\n",
    "# define a seed for reproducibility\n",
    "seed = 1\n",
    "np.random.seed = seed\n",
    "np.random.shuffle(messages)\n",
    "\n",
    "# call find_features function for each SMS message\n",
    "featuresets = [(find_features(text), label) for (text, label) in messages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can split the featuresets into training and testing datasets using sklearn\n",
    "from sklearn import model_selection\n",
    "\n",
    "# split the data into training and testing datasets\n",
    "training, testing = model_selection.train_test_split(featuresets, test_size = 0.25, random_state=seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3396\n",
      "1132\n"
     ]
    }
   ],
   "source": [
    "print(len(training))\n",
    "print(len(testing))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Scikit-Learn Classifiers with NLTK\n",
    "\n",
    "Now that we have our dataset, we can start building algorithms! Let's start with a simple linear support vector classifier, then expand to other algorithms. We'll need to import each algorithm we plan on using from sklearn.  We also need to import some performance metrics, such as accuracy_score and classification_report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy: 81.97879858657244\n"
     ]
    }
   ],
   "source": [
    "# We can use sklearn algorithms in NLTK\n",
    "from nltk.classify.scikitlearn import SklearnClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "model = SklearnClassifier(SVC(kernel = 'linear'))\n",
    "\n",
    "# train the model on the training data\n",
    "model.train(training)\n",
    "\n",
    "# and test on the testing dataset!\n",
    "accuracy = nltk.classify.accuracy(model, testing)*100\n",
    "print(\"SVC Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(2002)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 85.68904593639576\n",
      "Gradient Boosting Accuracy: 87.63250883392226\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 85.15901060070671\n",
      "SVM Linear Accuracy: 81.97879858657244\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Define models to train\n",
    "#names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Gradient Boosting\",\"Logistic Regression\", \"SGD Classifier\",\n",
    "       #  \"Naive Bayes\", \"SVM Linear\"]\n",
    "names = [\"Random Forest\", \"Gradient Boosting\", \"Logistic Regression\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "    #KNeighborsClassifier(),\n",
    "   #DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    GradientBoostingClassifier(),\n",
    "    LogisticRegression(),\n",
    "    #SGDClassifier(max_iter = 100),\n",
    "    #MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "\n",
    "for name, model in models:\n",
    "    nltk_model = SklearnClassifier(model)\n",
    "    nltk_model.train(training)\n",
    "    accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "    print(\"{} Accuracy: {}\".format(name, accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Voting Classifier: Accuracy: 81.97879858657244\n"
     ]
    }
   ],
   "source": [
    "# Ensemble methods - Voting classifier\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#names = [\"K Nearest Neighbors\", \"Decision Tree\", \"Random Forest\", \"Logistic Regression\", \"SGD Classifier\",\n",
    "       #  \"Naive Bayes\", \"SVM Linear\"]\n",
    "\n",
    "classifiers = [\n",
    "  #  KNeighborsClassifier(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "   LogisticRegression(),\n",
    "   # SGDClassifier(max_iter = 100),\n",
    "   # MultinomialNB(),\n",
    "    SVC(kernel = 'linear')\n",
    "]\n",
    "\n",
    "models = zip(names, classifiers)\n",
    "models = list(models)\n",
    "nltk_ensemble = SklearnClassifier(VotingClassifier(estimators = models, voting = 'hard', n_jobs = -1))\n",
    "nltk_ensemble.train(training)\n",
    "accuracy = nltk.classify.accuracy(nltk_model, testing)*100\n",
    "print(\"Voting Classifier: Accuracy: {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make class label prediction for testing set\n",
    "txt_features, labels = zip(*testing)\n",
    "\n",
    "prediction = nltk_ensemble.classify_many(txt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.55      0.59       210\n",
      "           1       0.90      0.93      0.91       922\n",
      "\n",
      "    accuracy                           0.86      1132\n",
      "   macro avg       0.77      0.74      0.75      1132\n",
      "weighted avg       0.85      0.86      0.85      1132\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">predicted</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>continuum</th>\n",
       "      <th>line</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">actual</th>\n",
       "      <th>continuum</th>\n",
       "      <td>116</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>line</th>\n",
       "      <td>68</td>\n",
       "      <td>854</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 predicted     \n",
       "                 continuum line\n",
       "actual continuum       116   94\n",
       "       line             68  854"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print a confusion matrix and a classification report\n",
    "print(classification_report(labels, prediction))\n",
    "\n",
    "pd.DataFrame(\n",
    "    confusion_matrix(labels, prediction),\n",
    "    index = [['actual', 'actual'], ['continuum', 'line']],\n",
    "    columns = [['predicted', 'predicted'], ['continuum', 'line']])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
